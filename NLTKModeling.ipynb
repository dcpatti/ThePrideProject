{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference articles\n",
    "\n",
    "#https://towardsdatascience.com/testing-the-waters-with-nltk-3600574f891c\n",
    "#https://github.com/samiramunir/Simple-Sentiment-Analysis-using-NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring in all the required modules\n",
    "\n",
    "import nltk\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "import string, re\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the CSV of tweets, take just the 'text' column, and save it back to disk\n",
    "df1= pd.read_csv('master_csv/tweet_df.csv')\n",
    "df2=df1[[\"text\"]]\n",
    "df2.to_csv('master_csv/text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = df2[[\"text\"]]\n",
    "sentences = sent_tokenize(str(speech))\n",
    "#we are taking the column and casting it as a string to prepare to feed it to the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Finally doing the thing\\n2      27time WorldSeries  Champions NewYork  Yan\\n3      Are you a woman', 'Do you like Angels  baseball\\n4      Were proud to join MLB  in taking a stand ag\\n5       Angels  social media representative LITERALL\\n6      Noticed the Nationals  didnt mention  LGBTQ\\n7      To quote larrywilmore  saying the word gay \\n8      The Red Sox are what Id call tier 1A', 'They QT\\n9      Huh', 'LGBTQ  youth', 'Where are the parents', 'W\\n10     This is exactly why  LGBTQ  has to do with bul\\n11     Everyone has their own beliefs', 'He doesnt sup\\n12     LGBTQ  is fighting for equal rights', 'Not bully\\n13     God gives all of us life and will judge us in \\n14     Please stop cyber bullying me', 'You are showin\\n15     UPDATED to reflect the date that the Angels  \\n16     https groupmaticseventseventSurfsun \\xa0 r\\n17     Aye Angels  can yall do this as well', 'lgbtq\\n18     Hello Angels  fans', 'The team is hosting its f\\n19     And he pitches for team USA as well', 'Gotta lov\\n20                                          Dover  Pride\\n21                                          Dover  Pride\\n22                                          Dover  Pride\\n23     There isnt much more annoying that the convers\\n24     lightworker oracle  some astrology oracle car\\n25     I am SO happy to see a resurgence in Good Omen\\n26                                         what a  angel\\n27     Yeah I agree we dont want to wake our littl\\n28     https musicyoutubecomwatchvFAPned jKIW\\n29     I think your guardian  angel  tells you which \\n                                                  \\n10222  Yankees  Honor  Stonewall  Anniversary With Co\\n10223  Stonewall  Inn takes its place among  Yankees \\n10224  アメリカ合衆国で人気No8 Before the NY  Yankees  played t\\n10225  A new plaque at New Yorks  Yankee  Stadium pa\\n10226  Caller to Mike Francesas show is NOT happy ab\\n10227  An  LGBTQ  Pride plaque next to where  Yankee \\n10228  Theres a plaque for Nelson Mandela in monumen\\n10229  The  Yankees  putting a  LGBTQ  plaque in monu\\n10230  Yankees  Honor  Stonewall  Anniversary With Co\\n10231  Im sure youre okay with the 911 memorial in\\n10232  Stonewall  Inn takes its place among  Yankees \\n10233  Stonewall  Inn takes its place among  Yankees \\n10234  From the Yankees  to a school in Ohio that li\\n10235  The legendary  Stonewall  would that have made\\n10236  Yankees  unveil plaque commemorating  Stonewal\\n10237  Why do you stand up for  LGBTQ  people', 'Becaus\\n10238  Yankees  Honor  Stonewall  Anniversary With Co\\n10239  Same as me', 'Seems that  Stonewall  lost 500 \\n10240  This is interesting considering one of the tea\\n10241  New on All Heels on Deck rrnAs Pride  Mo\\n10242  To clarify Sheryl is also rightly critical of\\n10243  Sin ánimo de controversia y después de habre l\\n10244  HOW THE  LGBTQ  FAITHFUL CAN TACKLE THE ROMAN \\n10245  The Yankees  honor LGBTQ  with a Stonewall m\\n10246  NY  Yankees  Unveil  LGBTQ  Plaque In Monument\\n10247  Trump Attacks On  LGBTQ  Continue This Time i\\n10248  Via HuffPostQueer   Yankees  Honor  Stonewal\\n10249  Ive seen the data these orgs have collected on\\n10250  Queen madonna  with the  LGBTQ  flag with t\\n10251  NY  Yankees  Unveil  LGBTQ  Plaque In Monument\\n\\n10252 rows x 1 columns']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#create a function that removes the punctuation and return just the first 15 sentences (tweets)\n",
    "#based on this, we're going to need to clean the data more \n",
    "\n",
    "def remove_punctuation(sentence):\n",
    "    sentence = re.sub(r'[^\\w\\s]','', sentence)\n",
    "    return sentence\n",
    "cleaned_sent = [remove_punctuation(sentence) for sentence in sentences]\n",
    "partial_speech= cleaned_sent[1:15]\n",
    "print(partial_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Finally', 'doing', 'the', 'thing', '2', '27time', 'WorldSeries', 'Champions', 'NewYork', 'Yan', '3', 'Are', 'you', 'a', 'woman'], ['Do', 'you', 'like', 'Angels', 'baseball', '4', 'Were', 'proud', 'to', 'join', 'MLB', 'in', 'taking', 'a', 'stand', 'ag', '5', 'Angels', 'social', 'media', 'representative', 'LITERALL', '6', 'Noticed', 'the', 'Nationals', 'didnt', 'mention', 'LGBTQ', '7', 'To', 'quote', 'larrywilmore', 'saying', 'the', 'word', 'gay', '8', 'The', 'Red', 'Sox', 'are', 'what', 'Id', 'call', 'tier', '1A'], ['They', 'QT', '9', 'Huh'], ['LGBTQ', 'youth'], ['Where', 'are', 'the', 'parents'], ['W', '10', 'This', 'is', 'exactly', 'why', 'LGBTQ', 'has', 'to', 'do', 'with', 'bul', '11', 'Everyone', 'has', 'their', 'own', 'beliefs'], ['He', 'doesnt', 'sup', '12', 'LGBTQ', 'is', 'fighting', 'for', 'equal', 'rights'], ['Not', 'bully', '13', 'God', 'gives', 'all', 'of', 'us', 'life', 'and', 'will', 'judge', 'us', 'in', '14', 'Please', 'stop', 'cyber', 'bullying', 'me'], ['You', 'are', 'showin', '15', 'UPDATED', 'to', 'reflect', 'the', 'date', 'that', 'the', 'Angels', '16', 'https', 'groupmaticseventseventSurfsun', 'r', '17', 'Aye', 'Angels', 'can', 'yall', 'do', 'this', 'as', 'well'], ['lgbtq', '18', 'Hello', 'Angels', 'fans'], ['The', 'team', 'is', 'hosting', 'its', 'f', '19', 'And', 'he', 'pitches', 'for', 'team', 'USA', 'as', 'well'], ['Got', 'ta', 'lov', '20', 'Dover', 'Pride', '21', 'Dover', 'Pride', '22', 'Dover', 'Pride', '23', 'There', 'isnt', 'much', 'more', 'annoying', 'that', 'the', 'convers', '24', 'lightworker', 'oracle', 'some', 'astrology', 'oracle', 'car', '25', 'I', 'am', 'SO', 'happy', 'to', 'see', 'a', 'resurgence', 'in', 'Good', 'Omen', '26', 'what', 'a', 'angel', '27', 'Yeah', 'I', 'agree', 'we', 'dont', 'want', 'to', 'wake', 'our', 'littl', '28', 'https', 'musicyoutubecomwatchvFAPned', 'jKIW', '29', 'I', 'think', 'your', 'guardian', 'angel', 'tells', 'you', 'which', '10222', 'Yankees', 'Honor', 'Stonewall', 'Anniversary', 'With', 'Co', '10223', 'Stonewall', 'Inn', 'takes', 'its', 'place', 'among', 'Yankees', '10224', 'アメリカ合衆国で人気No8', 'Before', 'the', 'NY', 'Yankees', 'played', 't', '10225', 'A', 'new', 'plaque', 'at', 'New', 'Yorks', 'Yankee', 'Stadium', 'pa', '10226', 'Caller', 'to', 'Mike', 'Francesas', 'show', 'is', 'NOT', 'happy', 'ab', '10227', 'An', 'LGBTQ', 'Pride', 'plaque', 'next', 'to', 'where', 'Yankee', '10228', 'Theres', 'a', 'plaque', 'for', 'Nelson', 'Mandela', 'in', 'monumen', '10229', 'The', 'Yankees', 'putting', 'a', 'LGBTQ', 'plaque', 'in', 'monu', '10230', 'Yankees', 'Honor', 'Stonewall', 'Anniversary', 'With', 'Co', '10231', 'Im', 'sure', 'youre', 'okay', 'with', 'the', '911', 'memorial', 'in', '10232', 'Stonewall', 'Inn', 'takes', 'its', 'place', 'among', 'Yankees', '10233', 'Stonewall', 'Inn', 'takes', 'its', 'place', 'among', 'Yankees', '10234', 'From', 'the', 'Yankees', 'to', 'a', 'school', 'in', 'Ohio', 'that', 'li', '10235', 'The', 'legendary', 'Stonewall', 'would', 'that', 'have', 'made', '10236', 'Yankees', 'unveil', 'plaque', 'commemorating', 'Stonewal', '10237', 'Why', 'do', 'you', 'stand', 'up', 'for', 'LGBTQ', 'people'], ['Becaus', '10238', 'Yankees', 'Honor', 'Stonewall', 'Anniversary', 'With', 'Co', '10239', 'Same', 'as', 'me'], ['Seems', 'that', 'Stonewall', 'lost', '500', '10240', 'This', 'is', 'interesting', 'considering', 'one', 'of', 'the', 'tea', '10241', 'New', 'on', 'All', 'Heels', 'on', 'Deck', 'rrnAs', 'Pride', 'Mo', '10242', 'To', 'clarify', 'Sheryl', 'is', 'also', 'rightly', 'critical', 'of', '10243', 'Sin', 'ánimo', 'de', 'controversia', 'y', 'después', 'de', 'habre', 'l', '10244', 'HOW', 'THE', 'LGBTQ', 'FAITHFUL', 'CAN', 'TACKLE', 'THE', 'ROMAN', '10245', 'The', 'Yankees', 'honor', 'LGBTQ', 'with', 'a', 'Stonewall', 'm', '10246', 'NY', 'Yankees', 'Unveil', 'LGBTQ', 'Plaque', 'In', 'Monument', '10247', 'Trump', 'Attacks', 'On', 'LGBTQ', 'Continue', 'This', 'Time', 'i', '10248', 'Via', 'HuffPostQueer', 'Yankees', 'Honor', 'Stonewal', '10249', 'Ive', 'seen', 'the', 'data', 'these', 'orgs', 'have', 'collected', 'on', '10250', 'Queen', 'madonna', 'with', 'the', 'LGBTQ', 'flag', 'with', 't', '10251', 'NY', 'Yankees', 'Unveil', 'LGBTQ', 'Plaque', 'In', 'Monument', '10252', 'rows', 'x', '1', 'columns']]\n"
     ]
    }
   ],
   "source": [
    "#now we tokenize\n",
    "partial_speech_words =[word_tokenize(sentence) for sentence in partial_speech]\n",
    "print(partial_speech_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 179\n",
      "first 30 stop words:\n",
      "['they', 'itself', 'up', 'very', 'out', 'needn', 'does', 'some', 'where', \"she's\", 'm', \"mightn't\", 'ain', 'than', 'haven', 'have', 'ma', 'from', 'so', 'of', 'd', \"should've\", \"that'll\", 'each', 'be', 'should', 'who', 'their', 'hers', \"needn't\"]\n"
     ]
    }
   ],
   "source": [
    "#load NLTK's stopwords dictionary\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "print('Number of stopwords:',len(stop_words))\n",
    "print(f'first 30 stop words:\\n{stop_words[:30]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words before removing stopwords:498\n",
      "number of words after removing stopwords:385\n",
      "[['Finally', 'thing', '2', '27time', 'WorldSeries', 'Champions', 'NewYork', 'Yan', '3', 'Are', 'woman'], ['Do', 'like', 'Angels', 'baseball', '4', 'Were', 'proud', 'join', 'MLB', 'taking', 'stand', 'ag', '5', 'Angels', 'social', 'media', 'representative', 'LITERALL', '6', 'Noticed', 'Nationals', 'didnt', 'mention', 'LGBTQ', '7', 'To', 'quote', 'larrywilmore', 'saying', 'word', 'gay', '8', 'The', 'Red', 'Sox', 'Id', 'call', 'tier', '1A'], ['They', 'QT', '9', 'Huh'], ['LGBTQ', 'youth'], ['Where', 'parents'], ['W', '10', 'This', 'exactly', 'LGBTQ', 'bul', '11', 'Everyone', 'beliefs'], ['He', 'doesnt', 'sup', '12', 'LGBTQ', 'fighting', 'equal', 'rights'], ['Not', 'bully', '13', 'God', 'gives', 'us', 'life', 'judge', 'us', '14', 'Please', 'stop', 'cyber', 'bullying'], ['You', 'showin', '15', 'UPDATED', 'reflect', 'date', 'Angels', '16', 'https', 'groupmaticseventseventSurfsun', 'r', '17', 'Aye', 'Angels', 'yall', 'well'], ['lgbtq', '18', 'Hello', 'Angels', 'fans'], ['The', 'team', 'hosting', 'f', '19', 'And', 'pitches', 'team', 'USA', 'well'], ['Got', 'ta', 'lov', '20', 'Dover', 'Pride', '21', 'Dover', 'Pride', '22', 'Dover', 'Pride', '23', 'There', 'isnt', 'much', 'annoying', 'convers', '24', 'lightworker', 'oracle', 'astrology', 'oracle', 'car', '25', 'I', 'SO', 'happy', 'see', 'resurgence', 'Good', 'Omen', '26', 'angel', '27', 'Yeah', 'I', 'agree', 'dont', 'want', 'wake', 'littl', '28', 'https', 'musicyoutubecomwatchvFAPned', 'jKIW', '29', 'I', 'think', 'guardian', 'angel', 'tells', '10222', 'Yankees', 'Honor', 'Stonewall', 'Anniversary', 'With', 'Co', '10223', 'Stonewall', 'Inn', 'takes', 'place', 'among', 'Yankees', '10224', 'アメリカ合衆国で人気No8', 'Before', 'NY', 'Yankees', 'played', '10225', 'A', 'new', 'plaque', 'New', 'Yorks', 'Yankee', 'Stadium', 'pa', '10226', 'Caller', 'Mike', 'Francesas', 'show', 'NOT', 'happy', 'ab', '10227', 'An', 'LGBTQ', 'Pride', 'plaque', 'next', 'Yankee', '10228', 'Theres', 'plaque', 'Nelson', 'Mandela', 'monumen', '10229', 'The', 'Yankees', 'putting', 'LGBTQ', 'plaque', 'monu', '10230', 'Yankees', 'Honor', 'Stonewall', 'Anniversary', 'With', 'Co', '10231', 'Im', 'sure', 'youre', 'okay', '911', 'memorial', '10232', 'Stonewall', 'Inn', 'takes', 'place', 'among', 'Yankees', '10233', 'Stonewall', 'Inn', 'takes', 'place', 'among', 'Yankees', '10234', 'From', 'Yankees', 'school', 'Ohio', 'li', '10235', 'The', 'legendary', 'Stonewall', 'would', 'made', '10236', 'Yankees', 'unveil', 'plaque', 'commemorating', 'Stonewal', '10237', 'Why', 'stand', 'LGBTQ', 'people'], ['Becaus', '10238', 'Yankees', 'Honor', 'Stonewall', 'Anniversary', 'With', 'Co', '10239', 'Same'], ['Seems', 'Stonewall', 'lost', '500', '10240', 'This', 'interesting', 'considering', 'one', 'tea', '10241', 'New', 'All', 'Heels', 'Deck', 'rrnAs', 'Pride', 'Mo', '10242', 'To', 'clarify', 'Sheryl', 'also', 'rightly', 'critical', '10243', 'Sin', 'ánimo', 'de', 'controversia', 'después', 'de', 'habre', 'l', '10244', 'HOW', 'THE', 'LGBTQ', 'FAITHFUL', 'CAN', 'TACKLE', 'THE', 'ROMAN', '10245', 'The', 'Yankees', 'honor', 'LGBTQ', 'Stonewall', '10246', 'NY', 'Yankees', 'Unveil', 'LGBTQ', 'Plaque', 'In', 'Monument', '10247', 'Trump', 'Attacks', 'On', 'LGBTQ', 'Continue', 'This', 'Time', '10248', 'Via', 'HuffPostQueer', 'Yankees', 'Honor', 'Stonewal', '10249', 'Ive', 'seen', 'data', 'orgs', 'collected', '10250', 'Queen', 'madonna', 'LGBTQ', 'flag', '10251', 'NY', 'Yankees', 'Unveil', 'LGBTQ', 'Plaque', 'In', 'Monument', '10252', 'rows', 'x', '1', 'columns']]\n"
     ]
    }
   ],
   "source": [
    "#create a function to remove the stopwords from the sentences\n",
    "\n",
    "def remove_stopword(sentence):\n",
    "    return [w for w in sentence if not w in stop_words]\n",
    "\n",
    "filtered = [remove_stopword(s) for s in partial_speech_words]\n",
    "word_count = len([w for words in partial_speech_words for w in words])\n",
    "word_count2 = len([w for words in filtered for w in words])\n",
    "print(f'number of words before removing stopwords:{word_count}')\n",
    "print(f'number of words after removing stopwords:{word_count2}')\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Finally', 'RB'), ('thing', 'NN'), ('2', 'CD'), ('27time', 'CD'), ('WorldSeries', 'NNS'), ('Champions', 'NNP'), ('NewYork', 'NNP'), ('Yan', 'NNP'), ('3', 'CD'), ('Are', 'NNP'), ('woman', 'NN')], [('Do', 'VBP'), ('like', 'IN'), ('Angels', 'NNP'), ('baseball', 'NN'), ('4', 'CD'), ('Were', 'NNP'), ('proud', 'JJ'), ('join', 'NN'), ('MLB', 'NNP'), ('taking', 'VBG'), ('stand', 'VBP'), ('ag', '$'), ('5', 'CD'), ('Angels', 'NNP'), ('social', 'JJ'), ('media', 'NNS'), ('representative', 'VBP'), ('LITERALL', 'NNP'), ('6', 'CD'), ('Noticed', 'NNP'), ('Nationals', 'NNP'), ('didnt', 'POS'), ('mention', 'NN'), ('LGBTQ', 'NNP'), ('7', 'CD'), ('To', 'TO'), ('quote', 'VB'), ('larrywilmore', 'RB'), ('saying', 'VBG'), ('word', 'NN'), ('gay', 'NN'), ('8', 'CD'), ('The', 'DT'), ('Red', 'NNP'), ('Sox', 'NNP'), ('Id', 'NNP'), ('call', 'VB'), ('tier', 'JJR'), ('1A', 'CD')], [('They', 'PRP'), ('QT', 'VBP'), ('9', 'CD'), ('Huh', 'NNP')]]\n"
     ]
    }
   ],
   "source": [
    "#use NLTK to tag the parts of speech\n",
    "#print just one sentence as a sample\n",
    "\n",
    "POS = [nltk.pos_tag(tokenized_sent) for tokenized_sent in filtered]\n",
    "print(POS[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use nltk's concordance method to pull forward all instances of a chosen word\n",
    "\n",
    "#revisit this later as it's not central to this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use this sample datasaet to train our sentiment analysis\n",
    "#http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "import os\n",
    "files_pos = os.listdir('sample_data/train/pos')\n",
    "files_pos = [open('sample_data/train/pos/'+f, 'r', encoding=\"utf8\").read() for f in files_pos]\n",
    "files_neg = os.listdir('sample_data/train/neg')\n",
    "\n",
    "files_neg = [open('sample_data/train/neg/'+f, 'r',encoding=\"utf8\").read() for f in files_neg]\n",
    "\n",
    "#make empty dictionaries to hold the  data\n",
    "all_words = []\n",
    "\n",
    "documents = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  j is adject, r is adverb, and v is verb\n",
    "\n",
    "#allowed_word_types = [\"J\",\"R\",\"V\"]\n",
    "\n",
    "allowed_word_types = [\"J\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in  files_pos:\n",
    "\n",
    "    # create a list of tuples where the first element of each tuple is a review\n",
    "    # the second element is the label\n",
    "\n",
    "    documents.append( (p, \"pos\") )\n",
    "   # remove punctuations\n",
    "    cleaned = re.sub(r'[^(a-zA-Z)\\s]','', p)   \n",
    "\n",
    "    # tokenize \n",
    "\n",
    "    tokenized = word_tokenize(cleaned)   \n",
    "\n",
    "    # remove stopwords \n",
    "\n",
    "    stopped = [w for w in tokenized if not w in stop_words]   \n",
    "\n",
    "    # parts of speech tagging for each word \n",
    "\n",
    "    pos = nltk.pos_tag(stopped)\n",
    "\n",
    "    # make a list of  all adjectives identified by the allowed word types list above\n",
    "\n",
    "    for w in pos:\n",
    "\n",
    "        if w[1][0] in allowed_word_types:\n",
    "\n",
    "            all_words.append(w[0].lower())\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in files_neg:\n",
    "\n",
    "    # create a list of tuples where the first element of each tuple is a review\n",
    "\n",
    "    # the second element is the label\n",
    "\n",
    "    documents.append( (p, \"neg\") )  \n",
    "\n",
    "    # remove punctuations\n",
    "\n",
    "    cleaned = re.sub(r'[^(a-zA-Z)\\s]','', p)    \n",
    "\n",
    "    # tokenize \n",
    "\n",
    "    tokenized = word_tokenize(cleaned)    \n",
    "\n",
    "    # remove stopwords \n",
    "\n",
    "    stopped = [w for w in tokenized if not w in stop_words]    \n",
    "\n",
    "    # parts of speech tagging for each word \n",
    "\n",
    "    neg = nltk.pos_tag(stopped)\n",
    "\n",
    "   # make a list of  all adjectives identified by the allowed word types list above\n",
    "\n",
    "    for w in neg:\n",
    "\n",
    "        if w[1][0] in allowed_word_types:\n",
    "\n",
    "            all_words.append(w[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a frequency distribution of each adjectives.\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "\n",
    "\n",
    "# listing the 5000 most frequent words\n",
    "\n",
    "word_features = list(all_words.keys())[:5000]\n",
    "\n",
    "\n",
    "\n",
    "# function to create a dictionary of features for each review in the list document.\n",
    "\n",
    "# The keys are the words in word_features \n",
    "\n",
    "# The values of each key are either true or false for wether that feature appears in the review or not\n",
    "\n",
    "\n",
    "\n",
    "def find_features(document):\n",
    "\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    for w in word_features:\n",
    "\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "# Creating features for each review\n",
    "\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a frequency distribution of each adjectives.\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "# listing the 5000 most frequent words\n",
    "word_features = list(all_words.keys())[:5000]\n",
    "\n",
    "# function to create a dictionary of features for each review in the list document.\n",
    "# The keys are the words in word_features \n",
    "# The values of each key are either true or false for wether that feature appears in the review or not\n",
    "\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "# Creating features for each review\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "# Shuffling the documents \n",
    "random.shuffle(featuresets)\n",
    "\n",
    "training_set = featuresets[:20000]\n",
    "testing_set = featuresets[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 85.12571428571428\n",
      "Most Informative Features\n",
      "             unwatchable = True              neg : pos    =     25.9 : 1.0\n",
      "                    blah = True              neg : pos    =     16.2 : 1.0\n",
      "                 droning = True              neg : pos    =     12.6 : 1.0\n",
      "             manipulated = True              pos : neg    =     12.5 : 1.0\n",
      "               pointless = True              neg : pos    =     11.0 : 1.0\n",
      "                flawless = True              pos : neg    =     10.2 : 1.0\n",
      "             disgraceful = True              neg : pos    =     10.0 : 1.0\n",
      "               offscreen = True              neg : pos    =     10.0 : 1.0\n",
      "               laughable = True              neg : pos    =      9.8 : 1.0\n",
      "                   worst = True              neg : pos    =      9.8 : 1.0\n",
      "                   awful = True              neg : pos    =      9.6 : 1.0\n",
      "                 mutated = True              neg : pos    =      9.4 : 1.0\n",
      "              unoriginal = True              neg : pos    =      9.2 : 1.0\n",
      "                   lousy = True              neg : pos    =      9.0 : 1.0\n",
      "              infectious = True              pos : neg    =      8.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1_score(ground_truth, preds, labels =['neg','pos'], average ='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patti\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\patti\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False))>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MNB_clf = SklearnClassifier(MultinomialNB())\n",
    "\n",
    "MNB_clf.train(training_set)\n",
    "\n",
    "\n",
    "\n",
    "BNB_clf = SklearnClassifier(BernoulliNB())\n",
    "\n",
    "BNB_clf.train(training_set)\n",
    "\n",
    "\n",
    "\n",
    "LogReg_clf = SklearnClassifier(LogisticRegression())\n",
    "\n",
    "LogReg_clf.train(training_set)\n",
    "\n",
    "\n",
    "\n",
    "SGD_clf = SklearnClassifier(SGDClassifier())\n",
    "\n",
    "SGD_clf.train(training_set)\n",
    "\n",
    "\n",
    "\n",
    "SVC_clf = SklearnClassifier(SVC())\n",
    "\n",
    "SVC_clf.train(training_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "\n",
    "# Defininig the ensemble model class \n",
    "\n",
    "class EnsembleClassifier(ClassifierI):\n",
    "    \n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    # returns the classification based on majority of votes\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    # a simple measurement the degree of confidence in the classification \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patti\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:306: UserWarning: Trying to unpickle estimator MultinomialNB from version 0.20.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\patti\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:306: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.20.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\patti\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:306: UserWarning: Trying to unpickle estimator DictVectorizer from version 0.20.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\patti\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:306: UserWarning: Trying to unpickle estimator BernoulliNB from version 0.20.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\patti\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:306: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.20.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\patti\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:306: UserWarning: Trying to unpickle estimator SGDClassifier from version 0.20.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load all classifiers from the pickled files \n",
    "\n",
    "# function to load models given filepath\n",
    "def load_model(file_path): \n",
    "    classifier_f = open(file_path, \"rb\")\n",
    "    classifier = pickle.load(classifier_f)\n",
    "    classifier_f.close()\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# Original Naive Bayes Classifier\n",
    "ONB_Clf = load_model('pickled_algos/ONB_clf.pickle')\n",
    "\n",
    "# Multinomial Naive Bayes Classifier \n",
    "MNB_Clf = load_model('pickled_algos/MNB_clf.pickle')\n",
    "\n",
    "\n",
    "# Bernoulli  Naive Bayes Classifier \n",
    "BNB_Clf = load_model('pickled_algos/BNB_clf.pickle')\n",
    "\n",
    "# Logistic Regression Classifier \n",
    "LogReg_Clf = load_model('pickled_algos/LogReg_clf.pickle')\n",
    "\n",
    "# Stochastic Gradient Descent Classifier\n",
    "SGD_Clf = load_model('pickled_algos/SGD_clf.pickle')\n",
    " \n",
    "\n",
    "# Initializing the ensemble classifier \n",
    "ensemble_clf = EnsembleClassifier(ONB_Clf, MNB_Clf, BNB_Clf, LogReg_Clf, SGD_Clf)\n",
    "\n",
    "# List of only feature dictionary from the featureset list of tuples \n",
    "feature_list = [f[0] for f in testing_set]\n",
    "\n",
    "# Looping over each to classify each review\n",
    "ensemble_preds = [ensemble_clf.classify(features) for features in feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
